{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b217f82",
   "metadata": {},
   "source": [
    "# Worksheet 5 — Linear Regression from Scratch\n",
    "\n",
    "**Dataset:** `student.csv`\n",
    "\n",
    "## Objective\n",
    "To predict the marks obtained in **Writing** based on the marks of **Math** and **Reading**.\n",
    "\n",
    "## Submission Instructions\n",
    "- Submit a single notebook containing:\n",
    "  1. Clean and well-documented code.\n",
    "  2. Outputs and visualizations.\n",
    "  3. Detailed explanations and analysis for all steps.\n",
    "- Ensure all cells are executed before submission.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Assumptions\n",
    "- **No bias/intercept term**: We assume bias = 0 for simplicity.\n",
    "- The model is: $Y = W^T X$\n",
    "\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^d$ (weight vector)\n",
    "- $X \\in \\mathbb{R}^{d \\times n}$ (feature matrix)\n",
    "- $Y \\in \\mathbb{R}^n$ (target vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96424e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea68e3",
   "metadata": {},
   "source": [
    "## Step 1 — Data Understanding, Analysis and Preparation\n",
    "\n",
    "### To-Do 1: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a827c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the dataset\n",
    "DATA_PATH = 'student.csv'\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(DATA_PATH)\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {DATA_PATH!r}. Please place student.csv in the same folder as this notebook.\"\n",
    "    ) from e\n",
    "\n",
    "# 2. Display top 5 rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 5 ROWS:\")\n",
    "print(\"=\"*80)\n",
    "display(data.head())\n",
    "\n",
    "# 3. Display bottom 5 rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BOTTOM 5 ROWS:\")\n",
    "print(\"=\"*80)\n",
    "display(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d47927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Print dataset information\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET INFORMATION:\")\n",
    "print(\"=\"*80)\n",
    "data.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUES CHECK:\")\n",
    "print(\"=\"*80)\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA TYPES:\")\n",
    "print(\"=\"*80)\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f01839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Descriptive statistics\n",
    "print(\"=\"*80)\n",
    "print(\"DESCRIPTIVE STATISTICS:\")\n",
    "print(\"=\"*80)\n",
    "display(data.describe())\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORRELATION MATRIX:\")\n",
    "print(\"=\"*80)\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    display(data[numeric_cols].corr())\n",
    "    \n",
    "    # Visualize correlations\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(data[numeric_cols].corr(), annot=True, fmt='.3f', cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfa629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Split data into Features (X) and Target (Y)\n",
    "# Assuming the dataset has columns: Math, Reading, Writing\n",
    "# We want to predict Writing based on Math and Reading\n",
    "\n",
    "# Check if expected columns exist\n",
    "expected_cols = ['Math', 'Reading', 'Writing']\n",
    "if all(col in data.columns for col in expected_cols):\n",
    "    X_full = data[['Math', 'Reading']].values  # Features\n",
    "    Y_full = data['Writing'].values  # Target\n",
    "    print(\"✓ Features (X) and Target (Y) successfully separated!\")\n",
    "    print(f\"  X shape: {X_full.shape} (samples × features)\")\n",
    "    print(f\"  Y shape: {Y_full.shape} (samples)\")\n",
    "else:\n",
    "    print(f\"Warning: Expected columns {expected_cols} not found.\")\n",
    "    print(f\"Available columns: {list(data.columns)}\")\n",
    "    # Fallback: use all numeric columns except the last as features\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    X_full = numeric_data.iloc[:, :-1].values\n",
    "    Y_full = numeric_data.iloc[:, -1].values\n",
    "    print(f\"Using fallback: X shape {X_full.shape}, Y shape {Y_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932daacc",
   "metadata": {},
   "source": [
    "### To-Do 2: Create Matrices\n",
    "\n",
    "We define:\n",
    "- **Weight matrix** $W \\in \\mathbb{R}^d$ where $d$ is the number of features\n",
    "- **Feature matrix** $X \\in \\mathbb{R}^{d \\times n}$ where $n$ is the number of samples\n",
    "- **Target vector** $Y \\in \\mathbb{R}^n$\n",
    "\n",
    "**Note:** No bias column (column of 1s) is added since we assume bias = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf5a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix dimensions\n",
    "n_samples, n_features = X_full.shape\n",
    "print(f\"Number of samples (n): {n_samples}\")\n",
    "print(f\"Number of features (d): {n_features}\")\n",
    "print(f\"\\nMatrix shapes:\")\n",
    "print(f\"  X: {X_full.shape} (n_samples × n_features)\")\n",
    "print(f\"  Y: {Y_full.shape} (n_samples,)\")\n",
    "print(f\"  W will be: ({n_features},) after initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980056d",
   "metadata": {},
   "source": [
    "### To-Do 3: Train-Test Split (from scratch)\n",
    "\n",
    "Split the dataset into training (70% or 80%) and test (30% or 20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eaf365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, Y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into training and test sets from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        Y: Target vector (n_samples,)\n",
    "        test_size: Proportion of data for testing (default 0.2 = 20%)\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, Y_train, Y_test\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Create shuffled indices\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split point\n",
    "    test_samples = int(n_samples * test_size)\n",
    "    train_samples = n_samples - test_samples\n",
    "    \n",
    "    # Split indices\n",
    "    train_indices = indices[:train_samples]\n",
    "    test_indices = indices[train_samples:]\n",
    "    \n",
    "    # Split data\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    Y_test = Y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Perform the split (80% train, 20% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_full, Y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT COMPLETED:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training set: X_train={X_train.shape}, Y_train={Y_train.shape}\")\n",
    "print(f\"Test set:     X_test={X_test.shape}, Y_test={Y_test.shape}\")\n",
    "print(f\"\\nSplit ratio: {100*(1-0.2):.0f}% train, {100*0.2:.0f}% test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83f21a",
   "metadata": {},
   "source": [
    "## Step 2 — Build a Cost Function\n",
    "\n",
    "The cost function for regression is the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$L(w) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_{pred}^{(i)} - y_i)^2$$\n",
    "\n",
    "where $y_{pred}(w) = W^T X$\n",
    "\n",
    "### To-Do 4: Implement Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error (MSE) cost function.\n",
    "    \n",
    "    Parameters:\n",
    "        X: Feature matrix (n_samples, n_features)\n",
    "        Y: Target vector (n_samples,)\n",
    "        W: Weight vector (n_features,)\n",
    "    \n",
    "    Returns:\n",
    "        cost: Mean squared error\n",
    "    \"\"\"\n",
    "    n = len(Y)  # Number of samples\n",
    "    \n",
    "    # Compute predictions: Y_pred = X @ W\n",
    "    Y_pred = np.dot(X, W)\n",
    "    \n",
    "    # Calculate squared errors\n",
    "    squared_errors = (Y_pred - Y) ** 2\n",
    "    \n",
    "    # Mean squared error (divided by 2n as per formula)\n",
    "    cost = np.sum(squared_errors) / (2 * n)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "print(\"✓ Cost function implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723f986",
   "metadata": {},
   "source": [
    "### To-Do 5: Test the Cost Function\n",
    "\n",
    "**Test Case:**\n",
    "Given:\n",
    "- $X = \\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 4 & 6 \\end{bmatrix}^T$ \n",
    "- $Y = \\begin{bmatrix} 3 & 7 & 11 \\end{bmatrix}^T$\n",
    "- $W = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "Expected: **cost = 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80568a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case for cost function\n",
    "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "Y_test = np.array([3, 7, 11])\n",
    "W_test = np.array([1, 1])\n",
    "\n",
    "cost = cost_function(X_test, Y_test, W_test)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COST FUNCTION TEST:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test input:\")\n",
    "print(f\"  X_test = \\n{X_test}\")\n",
    "print(f\"  Y_test = {Y_test}\")\n",
    "print(f\"  W_test = {W_test}\")\n",
    "print(f\"\\nPredictions: {np.dot(X_test, W_test)}\")\n",
    "print(f\"Cost function output: {cost}\")\n",
    "\n",
    "if cost == 0:\n",
    "    print(\"\\n✓ TEST PASSED! Proceed further.\")\n",
    "else:\n",
    "    print(f\"\\n✗ TEST FAILED! Expected 0, got {cost}\")\n",
    "    print(\"Something went wrong: Reimplement the cost function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5933c65",
   "metadata": {},
   "source": [
    "## Step 3 — Gradient Descent for Linear Regression\n",
    "\n",
    "**Objective:** Learn the parameters $W$ (weights) assuming bias $b = 0$.\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**Hypothesis:**\n",
    "$$h_w(x) = W^T x$$\n",
    "\n",
    "**Loss Function:**\n",
    "$$\\text{Loss} = (h_w(x) - y)^2$$\n",
    "\n",
    "**Gradient:**\n",
    "$$\\frac{\\partial \\text{Loss}}{\\partial w} = 2 \\cdot (h_w(x) - y) \\cdot x$$\n",
    "\n",
    "**Update Rule:**\n",
    "$$W^{(j+1)} = W^{(j)} - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ = learning rate\n",
    "- $m$ = number of training samples\n",
    "\n",
    "### To-Do 6: Implement Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, W, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): Feature matrix (m x n).\n",
    "        Y (numpy.ndarray): Target vector (m,).\n",
    "        W (numpy.ndarray): Initial guess for parameters (n,).\n",
    "        alpha (float): Learning rate.\n",
    "        iterations (int): Number of iterations for gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - W_update (numpy.ndarray): Updated parameters (n,).\n",
    "            - cost_history (list): History of cost values over iterations.\n",
    "    \"\"\"\n",
    "    # Initialize cost history\n",
    "    cost_history = [0] * iterations\n",
    "    \n",
    "    # Number of samples\n",
    "    m = len(Y)\n",
    "    \n",
    "    # Make a copy of W to avoid modifying the original\n",
    "    W_update = W.copy()\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        # Step 1: Hypothesis Values (Predictions)\n",
    "        Y_pred = np.dot(X, W_update)\n",
    "        \n",
    "        # Step 2: Difference between Hypothesis and Actual Y (Loss/Error)\n",
    "        loss = Y_pred - Y\n",
    "        \n",
    "        # Step 3: Gradient Calculation\n",
    "        # dw = (1/m) * X^T * (Y_pred - Y)\n",
    "        dw = (1/m) * np.dot(X.T, loss)\n",
    "        \n",
    "        # Step 4: Updating Values of W using Gradient\n",
    "        W_update = W_update - alpha * dw\n",
    "        \n",
    "        # Step 5: New Cost Value\n",
    "        cost = cost_function(X, Y, W_update)\n",
    "        cost_history[iteration] = cost\n",
    "    \n",
    "    return W_update, cost_history\n",
    "\n",
    "print(\"✓ Gradient descent function implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897763fd",
   "metadata": {},
   "source": [
    "### To-Do 7: Test Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffbcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random test data\n",
    "np.random.seed(0)  # For reproducibility\n",
    "X_gd_test = np.random.rand(100, 3)  # 100 samples, 3 features\n",
    "Y_gd_test = np.random.rand(100)\n",
    "W_gd_test = np.random.rand(3)  # Initial guess for parameters\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "# Test the gradient_descent function\n",
    "print(\"=\"*80)\n",
    "print(\"GRADIENT DESCENT TEST:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training on random data: {X_gd_test.shape[0]} samples, {X_gd_test.shape[1]} features\")\n",
    "print(f\"Learning rate (alpha): {alpha}\")\n",
    "print(f\"Iterations: {iterations}\")\n",
    "print(f\"\\nInitial weights: {W_gd_test}\")\n",
    "print(f\"Initial cost: {cost_function(X_gd_test, Y_gd_test, W_gd_test):.6f}\")\n",
    "\n",
    "final_params, cost_history = gradient_descent(X_gd_test, Y_gd_test, W_gd_test, alpha, iterations)\n",
    "\n",
    "# Print the final parameters and cost history\n",
    "print(f\"\\n✓ Gradient descent completed!\")\n",
    "print(f\"Final Parameters: {final_params}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.6f}\")\n",
    "print(f\"\\nCost History (first 10 iterations): {[f'{c:.6f}' for c in cost_history[:10]]}\")\n",
    "print(f\"Cost History (last 10 iterations): {[f'{c:.6f}' for c in cost_history[-10:]]}\")\n",
    "\n",
    "# Visualize cost over iterations\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cost_history, linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function vs Iterations (Gradient Descent Test)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2f512",
   "metadata": {},
   "source": [
    "## Step 4 — Evaluate the Model\n",
    "\n",
    "### To-Do 8: Implement RMSE (Root Mean Squared Error)\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341cf625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Root Mean Squared Error.\n",
    "    \n",
    "    Input Arguments:\n",
    "        Y: Array of actual (target) dependent variables.\n",
    "        Y_pred: Array of predicted dependent variables.\n",
    "    \n",
    "    Output Arguments:\n",
    "        rmse: Root Mean Square Error.\n",
    "    \"\"\"\n",
    "    # Calculate squared differences\n",
    "    squared_diff = (Y - Y_pred) ** 2\n",
    "    \n",
    "    # Mean of squared differences\n",
    "    mean_squared_error = np.mean(squared_diff)\n",
    "    \n",
    "    # Square root\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "print(\"✓ RMSE function implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eca280",
   "metadata": {},
   "source": [
    "### To-Do 9: Implement R² (Coefficient of Determination)\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "where:\n",
    "- $SS_{res} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ (Sum of Squared Residuals)\n",
    "- $SS_{tot} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$ (Total Sum of Squares)\n",
    "- $\\bar{y}$ = mean of actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5cd3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the R-Squared (Coefficient of Determination).\n",
    "    \n",
    "    Input Arguments:\n",
    "        Y: Array of actual (target) dependent variables.\n",
    "        Y_pred: Array of predicted dependent variables.\n",
    "    \n",
    "    Output Arguments:\n",
    "        r2: R-Squared Error.\n",
    "    \"\"\"\n",
    "    # Calculate mean of actual values\n",
    "    mean_y = np.mean(Y)\n",
    "    \n",
    "    # Total Sum of Squares (SS_tot)\n",
    "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
    "    \n",
    "    # Residual Sum of Squares (SS_res)\n",
    "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
    "    \n",
    "    # R-squared\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return r2\n",
    "\n",
    "print(\"✓ R² function implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13324de9",
   "metadata": {},
   "source": [
    "## Step 5 — Main Function to Integrate All Steps\n",
    "\n",
    "### To-Do 10: Create Main Integration Function\n",
    "\n",
    "This function brings everything together:\n",
    "1. Load and prepare data\n",
    "2. Train-test split\n",
    "3. Initialize weights and hyperparameters\n",
    "4. Run gradient descent\n",
    "5. Make predictions\n",
    "6. Evaluate using RMSE and R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to integrate all steps:\n",
    "    - Load data\n",
    "    - Split into train/test\n",
    "    - Run gradient descent\n",
    "    - Evaluate model\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"MAIN EXECUTION: LINEAR REGRESSION FROM SCRATCH\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Load the dataset\n",
    "    data = pd.read_csv('student.csv')\n",
    "    print(f\"\\n✓ Dataset loaded: {data.shape[0]} samples, {data.shape[1]} columns\")\n",
    "    \n",
    "    # Step 2: Split the data into features (X) and target (Y)\n",
    "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
    "    Y = data['Writing'].values  # Target: Writing marks\n",
    "    print(f\"✓ Features: Math and Reading → Target: Writing\")\n",
    "    \n",
    "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    print(f\"✓ Train-test split: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "    \n",
    "    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
    "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
    "    alpha = 0.00001  # Learning rate\n",
    "    iterations = 1000  # Number of iterations for gradient descent\n",
    "    print(f\"✓ Hyperparameters: alpha={alpha}, iterations={iterations}\")\n",
    "    print(f\"✓ Initial weights: {W}\")\n",
    "    \n",
    "    # Step 5: Perform Gradient Descent\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TRAINING MODEL WITH GRADIENT DESCENT...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
    "    print(f\"✓ Training complete!\")\n",
    "    print(f\"  Initial cost: {cost_history[0]:.4f}\")\n",
    "    print(f\"  Final cost: {cost_history[-1]:.4f}\")\n",
    "    print(f\"  Cost reduction: {cost_history[0] - cost_history[-1]:.4f}\")\n",
    "    \n",
    "    # Step 6: Make predictions on the test set\n",
    "    Y_pred = np.dot(X_test, W_optimal)\n",
    "    print(f\"\\n✓ Predictions made on test set\")\n",
    "    \n",
    "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
    "    model_rmse = rmse(Y_test, Y_pred)\n",
    "    model_r2 = r2(Y_test, Y_pred)\n",
    "    \n",
    "    # Step 8: Output the results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL RESULTS:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Final Weights: {W_optimal}\")\n",
    "    print(f\"Cost History (First 10 iterations): {cost_history[:10]}\")\n",
    "    print(f\"RMSE on Test Set: {model_rmse:.4f}\")\n",
    "    print(f\"R-Squared on Test Set: {model_r2:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Visualization 1: Cost over iterations\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(cost_history, linewidth=2, color='blue')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Cost Function vs Iterations')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Visualization 2: Actual vs Predicted\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(Y_test, Y_pred, alpha=0.6, edgecolors='k')\n",
    "    plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "    plt.xlabel('Actual Writing Score')\n",
    "    plt.ylabel('Predicted Writing Score')\n",
    "    plt.title('Actual vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return W_optimal, cost_history, model_rmse, model_r2\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    W_optimal, cost_history, model_rmse, model_r2 = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30b25c",
   "metadata": {},
   "source": [
    "## To-Do 11 — Present Your Findings & Experimentation\n",
    "\n",
    "### Model Performance Analysis\n",
    "\n",
    "Let's analyze whether our model is **overfitting**, **underfitting**, or has **acceptable performance**:\n",
    "\n",
    "**Interpretation Guidelines:**\n",
    "- **R² Score:**\n",
    "  - Close to 1.0 → Excellent fit\n",
    "  - 0.7 - 0.9 → Good fit\n",
    "  - 0.5 - 0.7 → Moderate fit\n",
    "  - < 0.5 → Poor fit\n",
    "  \n",
    "- **RMSE:** \n",
    "  - Lower is better\n",
    "  - Compare to the range of the target variable\n",
    "  \n",
    "- **Overfitting indicators:**\n",
    "  - Very high training performance but poor test performance\n",
    "  - Large gap between train and test metrics\n",
    "  \n",
    "- **Underfitting indicators:**\n",
    "  - Poor performance on both training and test sets\n",
    "  - Model is too simple to capture the underlying pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c6c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set as well to compare\n",
    "Y_train_pred = np.dot(X_train, W_optimal)\n",
    "train_rmse = rmse(Y_train, Y_train_pred)\n",
    "train_r2 = r2(Y_train, Y_train_pred)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Set Performance:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R²:   {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  RMSE: {model_rmse:.4f}\")\n",
    "print(f\"  R²:   {model_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nPerformance Gap:\")\n",
    "print(f\"  RMSE difference: {abs(train_rmse - model_rmse):.4f}\")\n",
    "print(f\"  R² difference:   {abs(train_r2 - model_r2):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANALYSIS:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Provide analysis\n",
    "if model_r2 > 0.7:\n",
    "    print(\"✓ Good model performance (R² > 0.7)\")\n",
    "elif model_r2 > 0.5:\n",
    "    print(\"⚠ Moderate model performance (R² between 0.5 and 0.7)\")\n",
    "else:\n",
    "    print(\"✗ Poor model performance (R² < 0.5)\")\n",
    "\n",
    "if abs(train_r2 - model_r2) < 0.1:\n",
    "    print(\"✓ Similar train/test performance → No significant overfitting\")\n",
    "else:\n",
    "    print(\"⚠ Gap between train/test performance → May indicate overfitting\")\n",
    "\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1283655",
   "metadata": {},
   "source": [
    "### Experiment with Different Learning Rates\n",
    "\n",
    "Let's experiment with different learning rates to observe their effect on model convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e88bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different learning rates\n",
    "learning_rates = [0.000001, 0.00001, 0.0001, 0.001, 0.01]\n",
    "iterations_exp = 1000\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENTING WITH DIFFERENT LEARNING RATES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for alpha_exp in learning_rates:\n",
    "    # Initialize weights\n",
    "    W_init = np.zeros(X_train.shape[1])\n",
    "    \n",
    "    # Run gradient descent\n",
    "    W_final, cost_hist = gradient_descent(X_train, Y_train, W_init, alpha_exp, iterations_exp)\n",
    "    \n",
    "    # Make predictions\n",
    "    Y_pred_exp = np.dot(X_test, W_final)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse_exp = rmse(Y_test, Y_pred_exp)\n",
    "    r2_exp = r2(Y_test, Y_pred_exp)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'learning_rate': alpha_exp,\n",
    "        'final_cost': cost_hist[-1],\n",
    "        'rmse': rmse_exp,\n",
    "        'r2': r2_exp,\n",
    "        'cost_history': cost_hist\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nLearning Rate: {alpha_exp:.6f}\")\n",
    "    print(f\"  Initial Cost: {cost_hist[0]:.4f}\")\n",
    "    print(f\"  Final Cost:   {cost_hist[-1]:.4f}\")\n",
    "    print(f\"  RMSE:         {rmse_exp:.4f}\")\n",
    "    print(f\"  R²:           {r2_exp:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of different learning rates\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    ax = axes[idx]\n",
    "    ax.plot(result['cost_history'], linewidth=2)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Cost')\n",
    "    ax.set_title(f\"α = {result['learning_rate']:.6f}\\nFinal R² = {result['r2']:.4f}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide the last subplot if we have an odd number\n",
    "if len(results) < len(axes):\n",
    "    axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Cost Function Convergence for Different Learning Rates', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison\n",
    "summary_df = pd.DataFrame([{\n",
    "    'Learning Rate': r['learning_rate'],\n",
    "    'Final Cost': f\"{r['final_cost']:.4f}\",\n",
    "    'RMSE': f\"{r['rmse']:.4f}\",\n",
    "    'R²': f\"{r['r2']:.4f}\"\n",
    "} for r in results])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY TABLE:\")\n",
    "print(\"=\"*80)\n",
    "display(summary_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
