{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3de6529",
   "metadata": {},
   "source": [
    "## 1. Understanding Classification Task\n",
    "\n",
    "### Classification Problem Formulation:\n",
    "Classification is a supervised learning problem, where we are given a training dataset of the form:\n",
    "\n",
    "$$D = \\{(X_i, Y_i) | x_i \\in \\mathbb{R}^d, y_i \\in \\{1, 2, ..., K\\}\\}_{i=1}^{n}$$\n",
    "\n",
    "where:\n",
    "- $x_i = [x_{i1}, x_{i2}, ..., x_{id}]$: is the feature vector for the $i^{th}$ sample.\n",
    "- $y_i$: is the target class label for the $i^{th}$ sample, belonging to one of K distinct classes.\n",
    "- $d$: is the number of features in each input vector.\n",
    "- $K$: is the number of classes in the classification problem.\n",
    "\n",
    "The goal is to learn a hypothesis function that maps input features to discrete class labels:\n",
    "\n",
    "$$f : \\mathbb{R}^d \\rightarrow \\{1, 2, ..., K\\}, f \\in M(\\text{Model Class})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ecaf98",
   "metadata": {},
   "source": [
    "## 2. Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced7dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d320982",
   "metadata": {},
   "source": [
    "## 3. Implementation of Sigmoid Regression from Scratch\n",
    "\n",
    "### 3.1 Building and Testing Helper Functions\n",
    "\n",
    "#### Task 1: Implementing Sigmoid Function\n",
    "\n",
    "The sigmoid function is given by:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "where: $x \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(x):\n",
    "    \"\"\"\n",
    "    Computes the logistic function applied to any value of x.\n",
    "    Arguments:\n",
    "    x: scalar or numpy array of any size.\n",
    "    Returns:\n",
    "    y: logistic function applied to x.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98b60d",
   "metadata": {},
   "source": [
    "#### Test Case for Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ca216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def test_logistic_function():\n",
    "    \"\"\"\n",
    "    Test cases for the logistic_function.\n",
    "    \"\"\"\n",
    "    # Test with scalar input\n",
    "    x_scalar = 0\n",
    "    expected_output_scalar = round(1 / (1 + np.exp(0)), 3) # Expected output: 0.5\n",
    "    assert round(logistic_function(x_scalar), 3) == expected_output_scalar, \"Test failed for scalar input\"\n",
    "    \n",
    "    # Test with positive scalar input\n",
    "    x_pos = 2\n",
    "    expected_output_pos = round(1 / (1 + np.exp(-2)), 3) # Expected output: ~0.881\n",
    "    assert round(logistic_function(x_pos), 3) == expected_output_pos, \"Test failed for positive scalar input\"\n",
    "    \n",
    "    # Test with negative scalar input\n",
    "    x_neg = -3\n",
    "    expected_output_neg = round(1 / (1 + np.exp(3)), 3) # Expected output: ~0.047\n",
    "    assert round(logistic_function(x_neg), 3) == expected_output_neg, \"Test failed for negative scalar input\"\n",
    "    \n",
    "    # Test with numpy array input\n",
    "    x_array = np.array([0, 2, -3])\n",
    "    expected_output_array = np.array([0.5, 0.881, 0.047]) # Adjusted expected values rounded to 3 decimals\n",
    "    # Use np.round to round the array element-wise and compare\n",
    "    assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), \"Test failed for numpy array input\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test case\n",
    "test_logistic_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443f011",
   "metadata": {},
   "source": [
    "#### Task 2: Implementing Log Loss Function\n",
    "\n",
    "For Sigmoid Regression and Binary Classification we use log-loss given by:\n",
    "\n",
    "$$L(y, \\hat{y}) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})$$\n",
    "\n",
    "where:\n",
    "- $y \\in \\{0, 1\\}$: True target value\n",
    "- $\\hat{y} = P(y = 1|x)$: Predicted target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes log loss for true target value y ={0 or 1} and predicted target value y' inbetween {0-1}.\n",
    "    Arguments:\n",
    "    y_true (scalar): true target value {0 or 1}.\n",
    "    y_pred (scalar): predicted taget value {0-1}.\n",
    "    Returns:\n",
    "    loss (float): loss/error value\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # Ensure y_pred is clipped to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "    loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd621427",
   "metadata": {},
   "source": [
    "#### Verifying the Intuition\n",
    "\n",
    "The basic intuition behind the log-loss function is: **the loss value should be minimum when our predicted probability values are closer to true target value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27bcdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function:\n",
    "y_true, y_pred = 0, 0.1\n",
    "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n",
    "print(\"+++++++++++++--------------------------++++++++++++++++++++++++\")\n",
    "y_true, y_pred = 1, 0.9\n",
    "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31ffe0",
   "metadata": {},
   "source": [
    "#### Test Case for Log Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbbdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_log_loss():\n",
    "    \"\"\"\n",
    "    Test cases for the log_loss function.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # Test case 1: Perfect prediction (y_true = 1, y_pred = 1)\n",
    "    y_true = 1\n",
    "    y_pred = 1\n",
    "    expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
    "    assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=1, y_pred=1)\"\n",
    "    \n",
    "    # Test case 2: Perfect prediction (y_true = 0, y_pred = 0)\n",
    "    y_true = 0\n",
    "    y_pred = 0\n",
    "    expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
    "    assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=0, y_pred=0)\"\n",
    "    \n",
    "    # Test case 3: Incorrect prediction (y_true = 1, y_pred = 0)\n",
    "    y_true = 1\n",
    "    y_pred = 0\n",
    "    try:\n",
    "        log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
    "    except ValueError:\n",
    "        pass # Test passed if ValueError is raised for log(0)\n",
    "    \n",
    "    # Test case 4: Incorrect prediction (y_true = 0, y_pred = 1)\n",
    "    y_true = 0\n",
    "    y_pred = 1\n",
    "    try:\n",
    "        log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
    "    except ValueError:\n",
    "        pass # Test passed if ValueError is raised for log(0)\n",
    "    \n",
    "    # Test case 5: Partially correct prediction\n",
    "    y_true = 1\n",
    "    y_pred = 0.8\n",
    "    expected_loss = -(1 * np.log(0.8)) - (0 * np.log(0.2)) # ~0.2231\n",
    "    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=1, y_pred=0.8)\"\n",
    "    \n",
    "    y_true = 0\n",
    "    y_pred = 0.2\n",
    "    expected_loss = -(0 * np.log(0.2)) - (1 * np.log(0.8)) # ~0.2231\n",
    "    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \"Test failed for partially correct prediction (y_true=0, y_pred=0.2)\"\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test case\n",
    "test_log_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df9ba2",
   "metadata": {},
   "source": [
    "#### Task 3: Implementing Cost Function\n",
    "\n",
    "The cost function is the average of loss function values calculated for each observation/data-point.\n",
    "\n",
    "$$\\text{cost}(y, \\hat{y}) = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} L(y_i, \\hat{y}_i)$$\n",
    "\n",
    "where:\n",
    "- $n$ â†’ number of observations/data-points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
    "    Args:\n",
    "    y_true (array_like, shape (n,)): array of true values (0 or 1)\n",
    "    y_pred (array_like, shape (n,)): array of predicted values (probability of y_pred being 1)\n",
    "    Returns:\n",
    "    cost (float): nonnegative cost corresponding to y_true and y_pred\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
    "    n = len(y_true)\n",
    "    loss_vec = log_loss(y_true, y_pred)\n",
    "    cost = np.sum(loss_vec) / n\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5222479",
   "metadata": {},
   "source": [
    "#### Testing the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def test_cost_function():\n",
    "    # Test case 1: Simple example with known expected cost\n",
    "    y_true = np.array([1, 0, 1])\n",
    "    y_pred = np.array([0.9, 0.1, 0.8])\n",
    "    # Expected output: Manually calculate cost for these values\n",
    "    # log_loss(y_true, y_pred) for each example\n",
    "    expected_cost = (-(1 * np.log(0.9)) - (1 - 1) * np.log(1 - 0.9) +\n",
    "                    -(0 * np.log(0.1)) - (1 - 0) * np.log(1 - 0.1) +\n",
    "                    -(1 * np.log(0.8)) - (1 - 1) * np.log(1 - 0.8)) / 3\n",
    "    \n",
    "    # Call the cost_function to get the result\n",
    "    result = cost_function(y_true, y_pred)\n",
    "    # Assert that the result is close to the expected cost with a tolerance of 1e-6\n",
    "    assert np.isclose(result, expected_cost, atol=1e-6), f\"Test failed: {result} != {expected_cost}\"\n",
    "    print(\"Test passed for simple case!\")\n",
    "\n",
    "# Run the test case\n",
    "test_cost_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e37ce0",
   "metadata": {},
   "source": [
    "#### Task 4: Cost Function for Sigmoid Regression\n",
    "\n",
    "We are estimating the following function:\n",
    "\n",
    "$$\\hat{y} = \\sigma(x \\cdot w^T + b) = \\frac{1}{1 + e^{-(x \\cdot w^T + b)}}$$\n",
    "\n",
    "The cost function $L(w, b)$ computes the average of the log loss for each training example:\n",
    "\n",
    "$$L(w, b) := C(y, \\hat{y} | X, w, b) = \\frac{1}{n} \\sum_{i=1}^{n} L\\left(y_i, \\frac{1}{1 + e^{-(x_i \\cdot w + b)}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cost function in terms of model parameters - using vectorization\n",
    "def costfunction_logreg(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function, given data and model parameters.\n",
    "    Args:\n",
    "    X (ndarray, shape (m,n)): data on features, m observations with n features.\n",
    "    y (array_like, shape (m,)): array of true values of target (0 or 1).\n",
    "    w (array_like, shape (n,)): weight parameters of the model.\n",
    "    b (float): bias parameter of the model.\n",
    "    Returns:\n",
    "    cost (float): nonnegative cost corresponding to y and y_pred.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    assert len(y) == n, \"Number of feature observations and number of target observations do not match.\"\n",
    "    assert len(w) == d, \"Number of features and number of weight parameters do not match.\"\n",
    "    # Compute z using np.dot\n",
    "    z = np.dot(X, w) + b # Matrix-vector multiplication and adding bias\n",
    "    # Compute predictions using logistic function (sigmoid)\n",
    "    y_pred = logistic_function(z)\n",
    "    # Compute the cost using the cost function\n",
    "    cost = cost_function(y, y_pred)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e87c07",
   "metadata": {},
   "source": [
    "#### Testing the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
    "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88d41a",
   "metadata": {},
   "source": [
    "#### Task 5: Implementing Gradient Descent for Training Sigmoid Regression\n",
    "\n",
    "##### Computing the Gradient\n",
    "\n",
    "The gradients of the binary cross-entropy log loss are:\n",
    "\n",
    "$$\\frac{\\partial \\text{Log Loss}}{\\partial w} = -\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) x_i$$\n",
    "\n",
    "$$\\frac{\\partial \\text{Log Loss}}{\\partial b} = -\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$$\n",
    "\n",
    "The weights $w$ and bias $b$ are updated as:\n",
    "\n",
    "$$w \\leftarrow w - \\alpha \\frac{\\partial \\text{Log Loss}}{\\partial w}, \\quad b \\leftarrow b - \\alpha \\frac{\\partial \\text{Log Loss}}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes gradients of the cost function with respect to model parameters.\n",
    "    Args:\n",
    "    X (ndarray, shape (n,d)): Input data, n observations with d features\n",
    "    y (array_like, shape (n,)): True labels (0 or 1)\n",
    "    w (array_like, shape (d,)): Weight parameters of the model\n",
    "    b (float): Bias parameter of the model\n",
    "    Returns:\n",
    "    grad_w (array_like, shape (d,)): Gradients of the cost function with respect to the weight parameters\n",
    "    grad_b (float): Gradient of the cost function with respect to the bias parameter\n",
    "    \"\"\"\n",
    "    n, d = X.shape # X has shape (n, d)\n",
    "    assert len(y) == n, f\"Expected y to have {n} elements, but got {len(y)}\"\n",
    "    assert len(w) == d, f\"Expected w to have {d} elements, but got {len(w)}\"\n",
    "    # Compute predictions using logistic function (sigmoid)\n",
    "    y_pred = 1 / (1 + np.exp(-(np.dot(X, w) + b)))\n",
    "    # Compute gradients\n",
    "    grad_w = -(1 / n) * np.dot(X.T, (y - y_pred)) # Gradient w.r.t weights, shape (d,)\n",
    "    grad_b = -(1 / n) * np.sum(y - y_pred) # Gradient w.r.t bias, scalar\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf3cc81",
   "metadata": {},
   "source": [
    "#### Simple Test for Compute Gradient Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test case\n",
    "X = np.array([[10, 20], [-10, 10]]) # shape (2, 2)\n",
    "y = np.array([1, 0]) # shape (2,)\n",
    "w = np.array([0.5, 1.5]) # shape (2,)\n",
    "b = 1 # scalar\n",
    "# Assertion tests\n",
    "try:\n",
    "    grad_w, grad_b = compute_gradient(X, y, w, b)\n",
    "    print(\"Gradients computed successfully.\")\n",
    "    print(f\"grad_w: {grad_w}\")\n",
    "    print(f\"grad_b: {grad_b}\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Assertion error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc72cd",
   "metadata": {},
   "source": [
    "#### Task 6: Gradient Descent for Sigmoid Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
    "    \"\"\"\n",
    "    Implements batch gradient descent to optimize logistic regression parameters.\n",
    "    Args:\n",
    "    X (ndarray, shape (n,d)): Data on features, n observations with d features\n",
    "    y (array_like, shape (n,)): True values of target (0 or 1)\n",
    "    w (array_like, shape (d,)): Initial weight parameters\n",
    "    b (float): Initial bias parameter\n",
    "    alpha (float): Learning rate\n",
    "    n_iter (int): Number of iterations\n",
    "    show_cost (bool): If True, displays cost every 100 iterations\n",
    "    show_params (bool): If True, displays parameters every 100 iterations\n",
    "    Returns:\n",
    "    w (array_like, shape (d,)): Optimized weight parameters\n",
    "    b (float): Optimized bias parameter\n",
    "    cost_history (list): List of cost values over iterations\n",
    "    params_history (list): List of parameters (w, b) over iterations\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    assert len(y) == n, \"Number of observations in X and y do not match\"\n",
    "    assert len(w) == d, \"Number of features in X and w do not match\"\n",
    "    cost_history = []\n",
    "    params_history = []\n",
    "    for i in range(n_iter):\n",
    "        # Compute gradients\n",
    "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
    "        # Update weights and bias\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "        # Compute cost\n",
    "        cost = costfunction_logreg(X, y, w, b)\n",
    "        # Store cost and parameters\n",
    "        cost_history.append(cost)\n",
    "        params_history.append((w.copy(), b))\n",
    "        # Optionally print cost and parameters\n",
    "        if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
    "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
    "        if show_params and (i % 100 == 0 or i == n_iter - 1):\n",
    "            print(f\"Iteration {i}: w = {w}, b = {b:.6f}\")\n",
    "    return w, b, cost_history, params_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e9e58",
   "metadata": {},
   "source": [
    "#### Testing the Gradient Descent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the gradient_descent function with sample data\n",
    "X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
    "y = np.array([1, 0]) # Shape (2,)\n",
    "w = np.zeros(X.shape[1]) # Shape (2,) - same as number of features\n",
    "b = 0.0 # Scalar\n",
    "alpha = 0.1 # Learning rate\n",
    "n_iter = 100000 # Number of iterations\n",
    "# Perform gradient descent\n",
    "w_out, b_out, cost_history, params_history = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=True, show_params=False)\n",
    "# Print final parameters and cost\n",
    "print(\"\\nFinal parameters:\")\n",
    "print(f\"w: {w_out}, b: {b_out}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60e667",
   "metadata": {},
   "source": [
    "#### Simple Assertion Test for Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple assertion test for gradient_descent\n",
    "def test_gradient_descent():\n",
    "    X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
    "    y = np.array([1, 0]) # Shape (2,)\n",
    "    w = np.zeros(X.shape[1]) # Shape (2,)\n",
    "    b = 0.0 # Scalar\n",
    "    alpha = 0.1 # Learning rate\n",
    "    n_iter = 100 # Number of iterations\n",
    "    # Run gradient descent\n",
    "    w_out, b_out, cost_history, _ = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=False)\n",
    "    # Assertions\n",
    "    assert len(cost_history) == n_iter, \"Cost history length does not match the number of iterations\"\n",
    "    assert w_out.shape == w.shape, \"Shape of output weights does not match the initial weights\"\n",
    "    assert isinstance(b_out, float), \"Bias output is not a float\"\n",
    "    assert cost_history[-1] < cost_history[0], \"Cost did not decrease over iterations\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cee7c",
   "metadata": {},
   "source": [
    "#### Visualizing Convergence of Cost During Gradient Descent\n",
    "\n",
    "This plot tracks how the cost decreases over iterations, providing insight into the convergence of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9f17b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting cost over iteration\n",
    "plt.figure(figsize = (9, 6))\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\", fontsize = 14)\n",
    "plt.ylabel(\"Cost\", fontsize = 14)\n",
    "plt.title(\"Cost vs Iteration\", fontsize = 14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d08746",
   "metadata": {},
   "source": [
    "#### Task 7: Decision/Prediction Function for Binary Classification\n",
    "\n",
    "We perform two tasks:\n",
    "1. **Prediction**: Using the trained weights and bias, calculate the probability for each sample.\n",
    "2. **Decision Boundary**: Convert predicted probability to binary class label using threshold $\\tau = 0.5$:\n",
    "\n",
    "$$\\hat{y} = \\begin{cases} 1 & \\text{if } y_{prob} \\geq \\tau \\\\ 0 & \\text{if } y_{prob} < \\tau \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847348bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def prediction(X, w, b, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predicts binary outcomes for given input features based on logistic regression parameters.\n",
    "    Arguments:\n",
    "    X (ndarray, shape (n,d)): Array of test independent variables (features) with n samples and d features.\n",
    "    w (ndarray, shape (d,)): Array of weights learned via gradient descent.\n",
    "    b (float): Bias learned via gradient descent.\n",
    "    threshold (float, optional): Classification threshold for predicting class labels. Default is 0.5.\n",
    "    Returns:\n",
    "    y_pred (ndarray, shape (n,)): Array of predicted dependent variable (binary class labels: 0 or 1).\n",
    "    \"\"\"\n",
    "    # Compute the predicted probabilities using the logistic function\n",
    "    z = np.dot(X, w) + b\n",
    "    y_test_prob = 1/(1 + np.exp(-z)) # z = wx + b\n",
    "    # Classify based on the threshold\n",
    "    y_pred = np.where(y_test_prob >= threshold, 1, 0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e072df",
   "metadata": {},
   "source": [
    "#### Test Case for Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction():\n",
    "    X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]]) # Shape (3, 2)\n",
    "    w_test = np.array([1.0, -1.0]) # Shape (2,)\n",
    "    b_test = 0.0 # Scalar bias\n",
    "    threshold = 0.5 # Default threshold\n",
    "    # Updated expected output\n",
    "    expected_output = np.array([0, 1, 1])\n",
    "    # Call the prediction function\n",
    "    y_pred = prediction(X_test, w_test, b_test, threshold)\n",
    "    # Assert that the output matches the expected output\n",
    "    assert np.array_equal(y_pred, expected_output), f\"Expected {expected_output}, but got {y_pred}\"\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "test_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa2465",
   "metadata": {},
   "source": [
    "#### Task 8: Evaluating Classifier\n",
    "\n",
    "This function computes confusion matrix, precision, recall, and F1-score from scratch based on predictions and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix, precision, recall, and F1-score for binary classification.\n",
    "    Arguments:\n",
    "    y_true (ndarray, shape (n,)): Ground truth binary labels (0 or 1).\n",
    "    y_pred (ndarray, shape (n,)): Predicted binary labels (0 or 1).\n",
    "    Returns:\n",
    "    metrics (dict): A dictionary containing confusion matrix, precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    # Initialize confusion matrix components\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1)) # True Positives\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0)) # True Negatives\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1)) # False Positives\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0)) # False Negatives\n",
    "    # Confusion matrix\n",
    "    confusion_matrix = np.array([[TN, FP],\n",
    "                                 [FN, TP]])\n",
    "    # Precision, recall, and F1-score\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0.0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0.0 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0.0 else 0.0\n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        \"confusion_matrix\": confusion_matrix,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104813bc",
   "metadata": {},
   "source": [
    "#### Testing Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ae6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 0, 1, 0, 1])\n",
    "y_pred = np.array([1, 0, 0, 0, 1])\n",
    "\n",
    "metrics = evaluate_classification(y_true, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", metrics[\"confusion_matrix\"])\n",
    "print(\"Precision:\", metrics[\"precision\"])\n",
    "print(\"Recall:\", metrics[\"recall\"])\n",
    "print(\"F1-score:\", metrics[\"f1_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46707736",
   "metadata": {},
   "source": [
    "## 4. Putting Helper Functions to Action - Sigmoid Regression on Real Dataset\n",
    "\n",
    "### Dataset Used: \"pima-indians-diabetes.data.csv\"\n",
    "\n",
    "### 4.1 Loading and Basic Data Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b98470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "data_pima_diabetes = pd.read_csv(url, names=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb9f4b",
   "metadata": {},
   "source": [
    "### 4.2 Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23408c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "columns_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "data_pima_diabetes[columns_to_clean] = data_pima_diabetes[columns_to_clean].replace(0, np.nan)\n",
    "data_pima_diabetes.fillna(data_pima_diabetes.median(), inplace=True)\n",
    "data_pima_diabetes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de748432",
   "metadata": {},
   "source": [
    "### 4.3 Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pima_diabetes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46559698",
   "metadata": {},
   "source": [
    "### 4.4 Train-Test Split and Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X = data_pima_diabetes.drop(columns=['Outcome']).values\n",
    "y = data_pima_diabetes['Outcome'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a930ee",
   "metadata": {},
   "source": [
    "### 4.5 Training the Sigmoid Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ce40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "w = np.zeros(X_train_scaled.shape[1])\n",
    "b = 0.0\n",
    "alpha = 0.1\n",
    "n_iter = 1000\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining Logistic Regression Model:\")\n",
    "w, b, cost_history, params_history = gradient_descent(X_train_scaled, y_train, w, b, alpha, n_iter, show_cost=True, show_params=False)\n",
    "\n",
    "# Plot cost history\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\", fontsize=14)\n",
    "plt.ylabel(\"Cost\", fontsize=14)\n",
    "plt.title(\"Cost vs Iteration\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed7169",
   "metadata": {},
   "source": [
    "### 4.6 Checking for Overfitting or Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "y_train_pred = prediction(X_train_scaled, w, b)\n",
    "y_test_pred = prediction(X_test_scaled, w, b)\n",
    "\n",
    "# Evaluate train and test performance\n",
    "train_cost = costfunction_logreg(X_train_scaled, y_train, w, b)\n",
    "test_cost = costfunction_logreg(X_test_scaled, y_test, w, b)\n",
    "print(f\"\\nTrain Loss (Cost): {train_cost:.4f}\")\n",
    "print(f\"Test Loss (Cost): {test_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82834d",
   "metadata": {},
   "source": [
    "### 4.7 Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on test data\n",
    "test_accuracy = np.mean(y_test_pred == y_test) * 100\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluation\n",
    "metrics = evaluate_classification(y_test, y_test_pred)\n",
    "confusion_matrix = metrics[\"confusion_matrix\"]\n",
    "precision = metrics[\"precision\"]\n",
    "recall = metrics[\"recall\"]\n",
    "f1_score = metrics[\"f1_score\"]\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\\n{confusion_matrix}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d54b1",
   "metadata": {},
   "source": [
    "### 4.8 Visualizing the Confusion Matrix (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefa97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.imshow(confusion_matrix, cmap='Blues')\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, confusion_matrix[i, j], ha='center', va='center', color='white', fontsize=20)\n",
    "plt.title('Confusion Matrix', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f252f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this worksheet, we successfully implemented Sigmoid/Logistic Regression from scratch using only NumPy and core Python libraries. We:\n",
    "\n",
    "1. Built helper functions including sigmoid function, log loss, cost function\n",
    "2. Implemented gradient computation and gradient descent optimization\n",
    "3. Created prediction and evaluation functions\n",
    "4. Applied our implementation to the Pima Indians Diabetes dataset\n",
    "5. Evaluated model performance using various metrics\n",
    "\n",
    "This hands-on implementation helps understand the mathematics and mechanics behind logistic regression, which is a fundamental algorithm in machine learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
