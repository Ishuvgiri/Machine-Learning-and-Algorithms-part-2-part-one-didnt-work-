{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53d736c",
   "metadata": {},
   "source": [
    "## 2. Getting Started with Pandas\n",
    "### 2.1 Building Blocks of Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23283596",
   "metadata": {},
   "source": [
    "#### 1. Data Structure - Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4acd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a simple Series\n",
    "data = [10, 20, 30, 40]\n",
    "series = pd.Series(data)\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec50ddd",
   "metadata": {},
   "source": [
    "#### 2. Data Structure - Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Index\n",
    "import pandas as pd\n",
    "series = pd.Series([10, 20, 30])\n",
    "print(series.index)\n",
    "# Output: RangeIndex(start=0, stop=3, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Defined:\n",
    "series = pd.Series([10, 20, 30], index=['a', 'b', 'c'])\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90763705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DateTimeIndex\n",
    "dates = pd.date_range('2023-01-01', periods=3)\n",
    "series = pd.Series([10, 20, 30], index=dates)\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access and Reset Index\n",
    "print(series.index)\n",
    "\n",
    "# Set or Reset Index\n",
    "series.index = ['x', 'y', 'z']  # Series\n",
    "\n",
    "# For DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2]}, index=['row1', 'row2'])\n",
    "df.reset_index(inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f3abf",
   "metadata": {},
   "source": [
    "#### 3. Data Structure - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fefe7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming in-built data structures - DataFrame\n",
    "# Style-1\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame({'Bob': ['I liked it.', 'It was awful'], 'Sue': ['Pretty good.', 'Bland.']})\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style-2\n",
    "df2 = pd.DataFrame({'Bob': ['I liked it.', 'It was awful.'], 'Sue': ['Pretty good.', 'Bland.']},\n",
    "                    index=['Product A', 'Product B'])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5816740",
   "metadata": {},
   "source": [
    "#### 4. DataFrames - Loading Data to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data from file\n",
    "import pandas as pd\n",
    "\n",
    "# path to your dataset must be given to builtin read_csv(\"Your path\") function.\n",
    "# dataset = pd.read_csv(\"/data/Week02/bank.csv\")\n",
    "# dataset.head()\n",
    "# dataset.tail()\n",
    "# dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9f84b",
   "metadata": {},
   "source": [
    "#### 5. DataFrames - Writing DataFrames to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41680dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data from file\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie'], 'City': ['New York', 'San Francisco', 'Los Angeles']}\n",
    "df = pd.DataFrame(data)  # creating a DataFrame\n",
    "\n",
    "# Writing DataFrame to csv.\n",
    "df.to_csv('output.csv', index=False)\n",
    "print(\"CSV file created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0df0b5",
   "metadata": {},
   "source": [
    "### 2.2 Basic Operation on Data: Data Inspection and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5799503",
   "metadata": {},
   "source": [
    "#### 1. First Data Inspection and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44778c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 70000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# View the first two rows\n",
    "print(\"First 2 rows:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# View the last row\n",
    "print(\"\\nLast row:\")\n",
    "print(df.tail(1))\n",
    "\n",
    "# DataFrame information\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check dimensions of the DataFrame\n",
    "print(f\"\\nThe DataFrame has {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "# Access the 'Age' column\n",
    "print(\"\\nAge column:\")\n",
    "print(df['Age'])\n",
    "\n",
    "# Select rows by numerical index\n",
    "print(\"\\nFirst row (using iloc):\")\n",
    "print(df.iloc[0])  # First row\n",
    "\n",
    "# Select rows by condition\n",
    "print(\"\\nRows where Age > 30:\")\n",
    "print(df.loc[df['Age'] > 30])  # Rows where Age > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26831bd",
   "metadata": {},
   "source": [
    "#### Understanding DataFrame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f164492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, None],\n",
    "    'Salary': [50000, 60000, 55000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae82f59",
   "metadata": {},
   "source": [
    "#### Understanding DataFrame.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, None],\n",
    "    'Salary': [50000, 60000, 55000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check summary statistics\n",
    "df.describe()  # Generate descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29df33",
   "metadata": {},
   "source": [
    "#### 2. Filtering and Modifying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 70000]\n",
    "})\n",
    "\n",
    "# Filter rows where Age > 28\n",
    "filtered_rows = df[df['Age'] > 28]\n",
    "print(\"Filtered rows (Age > 28):\")\n",
    "print(filtered_rows)\n",
    "\n",
    "# Select Specific Columns\n",
    "# Select only 'Name' and 'Salary' columns\n",
    "selected_columns = df[['Name', 'Salary']]\n",
    "print(\"\\nSelected columns (Name and Salary):\")\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Salary' column\n",
    "df_without_salary = df.drop(columns=['Salary'])\n",
    "print(\"DataFrame without Salary column:\")\n",
    "print(df_without_salary)\n",
    "\n",
    "# Drop the row with index 1 (Bob)\n",
    "df_without_row = df.drop(index=1)\n",
    "print(\"\\nDataFrame without row index 1:\")\n",
    "print(df_without_row)\n",
    "\n",
    "# Add a new column for Bonus\n",
    "df['Bonus'] = df['Salary'] * 0.1\n",
    "print(\"\\nDataFrame with Bonus column:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d06a65",
   "metadata": {},
   "source": [
    "### 2.3 Basic Operation on Data - Data Wrangling - Common Data Cleaning Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f7222",
   "metadata": {},
   "source": [
    "#### 1. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Some Missing Values\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()  # Load the Iris dataset\n",
    "iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], \n",
    "                       columns=iris['feature_names'] + ['target'])\n",
    "\n",
    "np.random.seed(42)  # Introduce missing values randomly\n",
    "mask = np.random.rand(*iris_df.shape) < 0.1  # 10%\n",
    "iris_df[mask] = np.nan\n",
    "\n",
    "print(\"Missing Values in Iris Dataset:\")\n",
    "print(iris_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with forward fill (ffill), mean, median, and 0\n",
    "iris_df_ffill = iris_df.ffill()\n",
    "iris_df_mean = iris_df.fillna(iris_df.mean())\n",
    "iris_df_median = iris_df.fillna(iris_df.median())\n",
    "iris_df_zero = iris_df.fillna(0)\n",
    "\n",
    "# Expand iris_df with filled columns\n",
    "iris_df_expanded = pd.concat([\n",
    "    iris_df, \n",
    "    iris_df_ffill.add_suffix('_ffill'), \n",
    "    iris_df_mean.add_suffix('_mean'),\n",
    "    iris_df_median.add_suffix('_median'),\n",
    "    iris_df_zero.add_suffix('_zero')\n",
    "], axis=1)\n",
    "\n",
    "# Display the head of the expanded DataFrame\n",
    "print(\"\\nDataset after Filling Missing Values:\")\n",
    "print(iris_df_expanded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520a208",
   "metadata": {},
   "source": [
    "#### 2. Some Common Operation performed for cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53847322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming Whitespaces\n",
    "df = pd.DataFrame({'Name': ['  Alice  ', '  Bob  '], 'Age': [25, 30]})\n",
    "df['Name'] = df['Name'].str.strip()\n",
    "print(\"After trimming whitespaces:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Datatype\n",
    "df = pd.DataFrame({'Age': ['25', '30', '35']})\n",
    "# Change 'Age' column datatype to integer\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "print(\"After changing datatype:\")\n",
    "print(df)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23234810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Columns\n",
    "df = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Age': [25, 30]})\n",
    "df = df.rename(columns={'Name': 'FullName', 'Age': 'Years'})\n",
    "print(\"After renaming columns:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Duplicates\n",
    "df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Alice'], 'Age': [25, 30, 25]})\n",
    "df = df.drop_duplicates()\n",
    "print(\"After removing duplicates:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02ccf2",
   "metadata": {},
   "source": [
    "#### 3. Data Transformation - DataFrame Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02'],\n",
    "    'City': ['Kathmandu', 'Pokhara', 'Kathmandu', 'Pokhara'],\n",
    "    'Temperature': [15, 18, 16, 19]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot: Reshape data to show cities as columns\n",
    "pivoted_df = df.pivot(index='Date', columns='City', values='Temperature')\n",
    "print(\"Pivoted DataFrame:\")\n",
    "print(pivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melting\n",
    "# Melt: Convert wide data back to long format\n",
    "melted_df = pd.melt(pivoted_df.reset_index(), id_vars=['Date'],\n",
    "                    var_name='City', value_name='Temperature')\n",
    "print(\"Melted DataFrame:\")\n",
    "print(melted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc88ba",
   "metadata": {},
   "source": [
    "#### 4. Data Transformation - Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()  # Load the Iris dataset\n",
    "iris_df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "\n",
    "# Min-Max Scaling using Pandas\n",
    "iris_minmax_scaled = (iris_df - iris_df.min()) / (iris_df.max() - iris_df.min())\n",
    "\n",
    "print(\"Original Iris DataFrame:\")\n",
    "print(iris_df.head())\n",
    "\n",
    "print(\"\\nMin-Max Scaled Iris DataFrame:\")\n",
    "print(iris_minmax_scaled.head())  # Display scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e6520",
   "metadata": {},
   "source": [
    "#### 5. Data Transformation - Handling Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal or Label Encoding\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame with ordinal categories\n",
    "df = pd.DataFrame({'Category': ['Low', 'Medium', 'High', 'Low', 'High']})\n",
    "\n",
    "# Ordinal encoding using map\n",
    "ordinal_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "df['Category_Ordinal'] = df['Category'].map(ordinal_mapping)\n",
    "print(\"Ordinal Encoding:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfecc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "import pandas as pd\n",
    "\n",
    "df_municipalities = pd.DataFrame({\n",
    "    'Municipality': ['Kathmandu', 'Bhaktapur', 'Lalitpur', 'Madhyapur Thimi', 'Kirtipur']\n",
    "})\n",
    "\n",
    "one_hot_encoding = pd.get_dummies(df_municipalities['Municipality'], prefix='Municipality')\n",
    "df_encoded = pd.concat([df_municipalities, one_hot_encoding], axis=1)\n",
    "print(\"One Hot Encoding:\")\n",
    "print(df_encoded)  # Display the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0ae1a",
   "metadata": {},
   "source": [
    "#### 6. Merging and Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "\n",
    "# Row-wise concatenation\n",
    "combined_rows = pd.concat([df1, df2], axis=0)\n",
    "print(\"Row-wise concatenation:\")\n",
    "print(combined_rows)\n",
    "\n",
    "# Column-wise concatenation\n",
    "combined_cols = pd.concat([df1, df2], axis=1)\n",
    "print(\"\\nColumn-wise concatenation:\")\n",
    "print(combined_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad13d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "# Sample DataFrames\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\n",
    "df2 = pd.DataFrame({'ID': [2, 3, 4], 'Score': [85, 90, 88]})\n",
    "\n",
    "# Inner join\n",
    "inner_merged = pd.merge(df1, df2, on='ID', how='inner')\n",
    "print(\"Inner Join:\")\n",
    "print(inner_merged)\n",
    "\n",
    "# Left join\n",
    "left_merged = pd.merge(df1, df2, on='ID', how='left')\n",
    "print(\"\\nLeft Join:\")\n",
    "print(left_merged)\n",
    "\n",
    "# Outer join\n",
    "outer_merged = pd.merge(df1, df2, on='ID', how='outer')\n",
    "print(\"\\nOuter Join:\")\n",
    "print(outer_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f605ab2",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. To-Do-Task\n",
    "### 3.1 Warming Up Exercises - Basic Inspection and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09add381",
   "metadata": {},
   "source": [
    "#### Problem 1 - Data Read, Write and Inspect\n",
    "**Dataset:** bank.csv\n",
    "\n",
    "Tasks:\n",
    "1. Load the provided dataset and import in pandas DataFrame.\n",
    "2. Check info of the DataFrame and identify following:\n",
    "   - (a) columns with dtypes=object\n",
    "   - (b) unique values of those columns.\n",
    "   - (c) check for the total number of null values in each column.\n",
    "3. Drop all the columns with dtypes object and store in new DataFrame, also write the DataFrame in \".csv\" with name \"bank_numeric_data.csv\"\n",
    "4. Read \"bank_numeric_data.csv\" and Find the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load bank.csv dataset\n",
    "bank_df = pd.read_csv('bank.csv')\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(bank_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Check DataFrame info\n",
    "print(\"\\nDataFrame Info:\")\n",
    "bank_df.info()\n",
    "\n",
    "# (a) Identify columns with dtype=object\n",
    "object_columns = bank_df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"\\n(a) Columns with dtypes=object:\")\n",
    "print(object_columns)\n",
    "\n",
    "# (b) Unique values of those columns\n",
    "print(\"\\n(b) Unique values of object columns:\")\n",
    "for col in object_columns:\n",
    "    print(f\"\\n{col}: {bank_df[col].nunique()} unique values\")\n",
    "    print(bank_df[col].unique())\n",
    "\n",
    "# (c) Check for total number of null values in each column\n",
    "print(\"\\n(c) Total number of null values in each column:\")\n",
    "print(bank_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Drop object columns and save to CSV\n",
    "bank_numeric_df = bank_df.select_dtypes(exclude=['object'])\n",
    "print(\"\\nDataFrame after dropping object columns:\")\n",
    "print(bank_numeric_df.head())\n",
    "\n",
    "# Write to CSV\n",
    "bank_numeric_df.to_csv('bank_numeric_data.csv', index=False)\n",
    "print(\"\\nNumeric data saved to 'bank_numeric_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Read the numeric CSV and find summary statistics\n",
    "bank_numeric_read = pd.read_csv('bank_numeric_data.csv')\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(bank_numeric_read.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f3b4b",
   "metadata": {},
   "source": [
    "#### Problem 2 - Data Imputations\n",
    "**Dataset:** medical_student.csv\n",
    "\n",
    "Tasks:\n",
    "1. Load the provided dataset and import in pandas DataFrame.\n",
    "2. Check info of the DataFrame and identify column with missing (null) values.\n",
    "3. For the column with missing values fill the values using various techniques we discussed above. Try to explain why did you select the particular methods for particular column.\n",
    "4. Check for any duplicate values present in Dataset and do necessary to manage the duplicate items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Load the dataset\n",
    "medical_df = pd.read_csv('medical_student.csv')\n",
    "print(\"Medical student dataset loaded successfully!\")\n",
    "print(medical_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Check DataFrame info and identify missing values\n",
    "print(\"\\nDataFrame Info:\")\n",
    "medical_df.info()\n",
    "\n",
    "print(\"\\nMissing values in each column:\")\n",
    "missing_values = medical_df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a51f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Fill missing values with appropriate techniques\n",
    "import numpy as np\n",
    "\n",
    "# Create a copy to work with\n",
    "medical_df_clean = medical_df.copy()\n",
    "\n",
    "# Example: Fill numerical columns with mean/median\n",
    "# For categorical columns, use mode or a specific category\n",
    "# You should adjust based on actual column names in the dataset\n",
    "\n",
    "for col in medical_df_clean.columns:\n",
    "    if medical_df_clean[col].isnull().sum() > 0:\n",
    "        if medical_df_clean[col].dtype in ['int64', 'float64']:\n",
    "            # For numerical columns, fill with median (more robust to outliers)\n",
    "            medical_df_clean[col].fillna(medical_df_clean[col].median(), inplace=True)\n",
    "            print(f\"\\nFilled '{col}' with median value (numerical column)\")\n",
    "        else:\n",
    "            # For categorical columns, fill with mode\n",
    "            medical_df_clean[col].fillna(medical_df_clean[col].mode()[0], inplace=True)\n",
    "            print(f\"\\nFilled '{col}' with mode value (categorical column)\")\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(medical_df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Check and remove duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")\n",
    "duplicates_count = medical_df_clean.duplicated().sum()\n",
    "print(duplicates_count)\n",
    "\n",
    "if duplicates_count > 0:\n",
    "    print(\"\\nRemoving duplicate rows...\")\n",
    "    medical_df_clean = medical_df_clean.drop_duplicates()\n",
    "    print(f\"Removed {duplicates_count} duplicate rows\")\n",
    "else:\n",
    "    print(\"\\nNo duplicate rows found!\")\n",
    "\n",
    "print(\"\\nFinal DataFrame shape:\")\n",
    "print(medical_df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af313dd",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2 Exercises - Data Cleaning and Transformations with \"Titanic Dataset\"\n",
    "**Dataset:** titanic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "titanic_df = pd.read_csv('titanic.csv')\n",
    "print(\"Titanic dataset loaded successfully!\")\n",
    "print(titanic_df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "titanic_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8913be",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "Create a DataFrame that is subsetted for the columns 'Name', 'Pclass', 'Sex', 'Age', 'Fare', and 'Survived'. Retain only those rows where 'Pclass' is equal to 1, representing first-class passengers. What is the mean, median, maximum value, and minimum value of the 'Fare' column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43131d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset DataFrame for specific columns\n",
    "titanic_subset = titanic_df[['Name', 'Pclass', 'Sex', 'Age', 'Fare', 'Survived']]\n",
    "\n",
    "# Filter for first-class passengers only\n",
    "first_class_df = titanic_subset[titanic_subset['Pclass'] == 1]\n",
    "\n",
    "print(\"First-class passengers DataFrame:\")\n",
    "print(first_class_df.head())\n",
    "\n",
    "# Calculate statistics for Fare column\n",
    "print(\"\\nFare Statistics for First-Class Passengers:\")\n",
    "print(f\"Mean: {first_class_df['Fare'].mean():.2f}\")\n",
    "print(f\"Median: {first_class_df['Fare'].median():.2f}\")\n",
    "print(f\"Maximum: {first_class_df['Fare'].max():.2f}\")\n",
    "print(f\"Minimum: {first_class_df['Fare'].min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce725fc1",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    "How many null values are contained in the 'Age' column in your subsetted DataFrame? Once you've found this out, drop them from your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values in Age column\n",
    "age_null_count = first_class_df['Age'].isnull().sum()\n",
    "print(f\"Number of null values in 'Age' column: {age_null_count}\")\n",
    "\n",
    "# Drop rows with null Age values\n",
    "first_class_df_clean = first_class_df.dropna(subset=['Age'])\n",
    "\n",
    "print(f\"\\nDataFrame shape before dropping nulls: {first_class_df.shape}\")\n",
    "print(f\"DataFrame shape after dropping nulls: {first_class_df_clean.shape}\")\n",
    "print(f\"Rows removed: {first_class_df.shape[0] - first_class_df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19fd36",
   "metadata": {},
   "source": [
    "#### Problem 3\n",
    "The 'Embarked' column in the Titanic dataset contains categorical data representing the ports of embarkation:\n",
    "- 'C' for Cherbourg\n",
    "- 'Q' for Queenstown\n",
    "- 'S' for Southampton\n",
    "\n",
    "Tasks:\n",
    "1. Use one-hot encoding to convert the 'Embarked' column into separate binary columns ('Embarked_C', 'Embarked_Q', 'Embarked_S').\n",
    "2. Add these new columns to the original DataFrame.\n",
    "3. Drop the original 'Embarked' column.\n",
    "4. Print the first few rows of the modified DataFrame to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the full dataset for this task\n",
    "titanic_encoded = titanic_df.copy()\n",
    "\n",
    "# One-hot encoding for 'Embarked' column\n",
    "embarked_encoded = pd.get_dummies(titanic_encoded['Embarked'], prefix='Embarked')\n",
    "\n",
    "# Add encoded columns to DataFrame\n",
    "titanic_encoded = pd.concat([titanic_encoded, embarked_encoded], axis=1)\n",
    "\n",
    "# Drop original 'Embarked' column\n",
    "titanic_encoded = titanic_encoded.drop(columns=['Embarked'])\n",
    "\n",
    "print(\"DataFrame after one-hot encoding 'Embarked' column:\")\n",
    "print(titanic_encoded.head())\n",
    "print(\"\\nColumns in modified DataFrame:\")\n",
    "print(titanic_encoded.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168032f8",
   "metadata": {},
   "source": [
    "#### Problem 4\n",
    "Compare the mean survival rates ('Survived') for the different groups in the 'Sex' column. Draw a visualization to show how the survival distributions vary by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0940f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean survival rates by sex\n",
    "survival_by_sex = titanic_df.groupby('Sex')['Survived'].mean()\n",
    "\n",
    "print(\"Mean Survival Rates by Sex:\")\n",
    "print(survival_by_sex)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar plot\n",
    "plt.subplot(1, 2, 1)\n",
    "survival_by_sex.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Mean Survival Rate by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Count plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=titanic_df, x='Sex', hue='Survived')\n",
    "plt.title('Survival Count by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Survived', labels=['No', 'Yes'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270191a",
   "metadata": {},
   "source": [
    "#### Problem 5\n",
    "Draw a visualization that breaks your visualization from Exercise 4 down by the port of embarkation ('Embarked'). In this instance, compare the ports 'C' (Cherbourg), 'Q' (Queenstown), and 'S' (Southampton)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e37d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate survival rates by sex and embarkation port\n",
    "survival_by_sex_embarked = titanic_df.groupby(['Sex', 'Embarked'])['Survived'].mean().unstack()\n",
    "\n",
    "print(\"Mean Survival Rates by Sex and Embarkation Port:\")\n",
    "print(survival_by_sex_embarked)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Grouped bar chart\n",
    "plt.subplot(1, 2, 1)\n",
    "survival_by_sex_embarked.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Survival Rate by Gender and Port of Embarkation')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.legend(title='Port', labels=['Cherbourg (C)', 'Queenstown (Q)', 'Southampton (S)'])\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Faceted count plot\n",
    "plt.subplot(1, 2, 2)\n",
    "titanic_clean = titanic_df.dropna(subset=['Embarked'])\n",
    "sns.barplot(data=titanic_clean, x='Embarked', y='Survived', hue='Sex')\n",
    "plt.title('Survival Rate by Port and Gender')\n",
    "plt.xlabel('Port of Embarkation')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.legend(title='Gender')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dbdab",
   "metadata": {},
   "source": [
    "#### Problem 6 {Optional}\n",
    "Show how the survival rates ('Survived') vary by age group and passenger class ('Pclass'). Break up the 'Age' column into five quantiles in your DataFrame, and then compare the means of 'Survived' by class and age group. Draw a visualization using any plotting library to represent this graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age groups using quantiles\n",
    "titanic_age = titanic_df.dropna(subset=['Age']).copy()\n",
    "\n",
    "# Create 5 age quantiles\n",
    "titanic_age['Age_Group'] = pd.qcut(titanic_age['Age'], q=5, labels=['Q1 (Youngest)', 'Q2', 'Q3', 'Q4', 'Q5 (Oldest)'])\n",
    "\n",
    "# Calculate survival rates by Pclass and Age_Group\n",
    "survival_by_class_age = titanic_age.groupby(['Pclass', 'Age_Group'])['Survived'].mean().unstack()\n",
    "\n",
    "print(\"Mean Survival Rates by Passenger Class and Age Group:\")\n",
    "print(survival_by_class_age)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Grouped bar chart\n",
    "plt.subplot(1, 2, 1)\n",
    "survival_by_class_age.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Survival Rate by Passenger Class and Age Group')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.legend(title='Age Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Heatmap\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(survival_by_class_age, annot=True, fmt='.2f', cmap='YlGnBu', cbar_kws={'label': 'Survival Rate'})\n",
    "plt.title('Survival Rate Heatmap: Class vs Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Passenger Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional faceted visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, pclass in enumerate([1, 2, 3]):\n",
    "    class_data = titanic_age[titanic_age['Pclass'] == pclass]\n",
    "    survival_by_age = class_data.groupby('Age_Group')['Survived'].mean()\n",
    "    \n",
    "    axes[i].bar(range(len(survival_by_age)), survival_by_age.values, color='steelblue')\n",
    "    axes[i].set_title(f'Class {pclass} Survival by Age Group')\n",
    "    axes[i].set_xlabel('Age Group')\n",
    "    axes[i].set_ylabel('Survival Rate')\n",
    "    axes[i].set_xticks(range(len(survival_by_age)))\n",
    "    axes[i].set_xticklabels(survival_by_age.index, rotation=45, ha='right')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f34eb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
