{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a42c60b",
   "metadata": {},
   "source": [
    "# Worksheet 4 — k-Nearest Neighbors (KNN) Classification from Scratch\n",
    "\n",
    "**Dataset:** `diabetes.csv`\n",
    "\n",
    "## Objective\n",
    "Perform classification using k-Nearest Neighbors (KNN) algorithm implemented from scratch (without scikit-learn).\n",
    "\n",
    "## Submission Instructions\n",
    "- Submit a single notebook containing:\n",
    "  1. Clean and well-documented code.\n",
    "  2. Outputs and visualizations.\n",
    "  3. Detailed explanations and analysis for all steps.\n",
    "- Ensure all cells are executed before submission.\n",
    "\n",
    "---\n",
    "\n",
    "## Problems Overview\n",
    "\n",
    "**Problem 1:** Implement KNN from scratch, handle missing data, evaluate on diabetes dataset  \n",
    "**Problem 2:** Compare performance on original vs scaled features  \n",
    "**Problem 3:** Experiment with different k values (1-15), plot accuracy and time  \n",
    "**Problem 4:** Discuss challenges and optimization strategies for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49edf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6120d3f",
   "metadata": {},
   "source": [
    "# Problem 1 — KNN Classification from Scratch\n",
    "\n",
    "## Step 1: Load the Dataset and Perform EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "DATA_PATH = 'diabetes.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {DATA_PATH!r}. Please place diabetes.csv in the same folder as this notebook.\"\n",
    "    ) from e\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FIRST 5 ROWS:\")\n",
    "print(\"=\"*80)\n",
    "display(df.head())\n",
    "\n",
    "# Display last few rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAST 5 ROWS:\")\n",
    "print(\"=\"*80)\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e27cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Exploratory Data Analysis (EDA)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET INFORMATION:\")\n",
    "print(\"=\"*80)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA TYPES:\")\n",
    "print(\"=\"*80)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUES:\")\n",
    "print(\"=\"*80)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS:\")\n",
    "print(\"=\"*80)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb33b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "target_col = 'Outcome' if 'Outcome' in df.columns else df.columns[-1]\n",
    "print(\"=\"*80)\n",
    "print(f\"TARGET VARIABLE: {target_col}\")\n",
    "print(\"=\"*80)\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df[target_col].value_counts(normalize=True))\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df[target_col].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title(f'Distribution of {target_col}')\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOP CORRELATIONS WITH TARGET:\")\n",
    "    print(\"=\"*80)\n",
    "    if target_col in corr_matrix.columns:\n",
    "        target_corr = corr_matrix[target_col].drop(target_col).sort_values(ascending=False)\n",
    "        print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f91c8",
   "metadata": {},
   "source": [
    "## Step 2: Handle Missing Data\n",
    "\n",
    "For the diabetes dataset, some features may have 0 values that represent missing data (e.g., Glucose, BloodPressure, BMI cannot realistically be 0). We'll handle these appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing data\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Columns that shouldn't have 0 values in diabetes dataset\n",
    "zero_as_missing = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "present_cols = [col for col in zero_as_missing if col in df_clean.columns]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"HANDLING MISSING DATA:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if present_cols:\n",
    "    print(f\"\\nReplacing 0 with NaN in: {present_cols}\")\n",
    "    for col in present_cols:\n",
    "        # Count zeros before replacement\n",
    "        zero_count = (df_clean[col] == 0).sum()\n",
    "        if zero_count > 0:\n",
    "            print(f\"  {col}: {zero_count} zeros found\")\n",
    "            df_clean.loc[df_clean[col] == 0, col] = np.nan\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING VALUES AFTER ZERO REPLACEMENT:\")\n",
    "print(\"=\"*80)\n",
    "missing_counts = df_clean.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])\n",
    "\n",
    "# Impute missing values with median\n",
    "numeric_features = df_clean.select_dtypes(include=[np.number]).columns.drop(target_col, errors='ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPUTING MISSING VALUES (using median):\")\n",
    "print(\"=\"*80)\n",
    "for col in numeric_features:\n",
    "    if df_clean[col].isnull().any():\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col].fillna(median_val, inplace=True)\n",
    "        print(f\"  {col}: imputed with median = {median_val:.2f}\")\n",
    "\n",
    "# Drop rows with missing target\n",
    "if df_clean[target_col].isnull().any():\n",
    "    print(f\"\\nDropping {df_clean[target_col].isnull().sum()} rows with missing target values\")\n",
    "    df_clean = df_clean.dropna(subset=[target_col])\n",
    "\n",
    "print(f\"\\n✓ Final dataset shape after cleaning: {df_clean.shape}\")\n",
    "print(f\"✓ No missing values remaining: {df_clean.isnull().sum().sum() == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6c441",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "\n",
    "Separate features (X) and target (y), then perform train-test split from scratch using 70-30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_clean.drop(columns=[target_col]).select_dtypes(include=[np.number]).values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# Ensure y is integer type\n",
    "if not np.issubdtype(y.dtype, np.integer):\n",
    "    if np.all(np.mod(y, 1) == 0):\n",
    "        y = y.astype(int)\n",
    "    else:\n",
    "        # Encode non-integer labels\n",
    "        unique_labels = np.unique(y)\n",
    "        label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        y = np.array([label_map[label] for label in y])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE MATRIX AND TARGET:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"X shape: {X.shape} (samples × features)\")\n",
    "print(f\"y shape: {y.shape} (samples)\")\n",
    "print(f\"Unique classes: {np.unique(y)}\")\n",
    "print(f\"Class counts: {dict(zip(*np.unique(y, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed73ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split from scratch (70-30 ratio)\n",
    "def train_test_split_scratch(X, y, test_ratio=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into training and test sets from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        test_ratio: Proportion for test set (default 0.3 = 30%)\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Shuffle indices\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split point\n",
    "    test_size = int(n_samples * test_ratio)\n",
    "    train_size = n_samples - test_size\n",
    "    \n",
    "    # Split\n",
    "    train_idx = indices[:train_size]\n",
    "    test_idx = indices[train_size:]\n",
    "    \n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "# Perform split\n",
    "X_train, X_test, y_train, y_test = train_test_split_scratch(X, y, test_ratio=0.3, random_state=42)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT (70-30):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training set: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Test set:     X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "print(f\"\\nTrain class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "print(f\"Test class distribution:  {dict(zip(*np.unique(y_test, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429dcd4",
   "metadata": {},
   "source": [
    "## Step 4: Implement KNN from Scratch\n",
    "\n",
    "We'll implement:\n",
    "1. **Euclidean distance** calculation\n",
    "2. **Predict single query** - find k nearest neighbors and vote\n",
    "3. **Predict all test samples** - batch prediction\n",
    "4. **Accuracy metric** - evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two points.\n",
    "    \n",
    "    Parameters:\n",
    "        point1: First point (numpy array)\n",
    "        point2: Second point (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        distance: Euclidean distance\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "\n",
    "def knn_predict_single(X_train, y_train, query_point, k):\n",
    "    \"\"\"\n",
    "    Predict class for a single query point using KNN.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: Training feature matrix\n",
    "        y_train: Training labels\n",
    "        query_point: Single test point to classify\n",
    "        k: Number of nearest neighbors\n",
    "    \n",
    "    Returns:\n",
    "        predicted_class: Predicted class label\n",
    "    \"\"\"\n",
    "    # Calculate distances from query point to all training points\n",
    "    distances = []\n",
    "    for i in range(len(X_train)):\n",
    "        dist = euclidean_distance(query_point, X_train[i])\n",
    "        distances.append((dist, y_train[i]))\n",
    "    \n",
    "    # Sort by distance and get k nearest neighbors\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest = distances[:k]\n",
    "    \n",
    "    # Get labels of k nearest neighbors\n",
    "    k_labels = [label for _, label in k_nearest]\n",
    "    \n",
    "    # Majority vote (use Counter for tie-breaking)\n",
    "    vote_counts = Counter(k_labels)\n",
    "    # If tie, choose the class that appears first among nearest neighbors\n",
    "    predicted_class = vote_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "def knn_predict(X_train, y_train, X_test, k):\n",
    "    \"\"\"\n",
    "    Predict classes for all test samples.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: Training feature matrix\n",
    "        y_train: Training labels\n",
    "        X_test: Test feature matrix\n",
    "        k: Number of nearest neighbors\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Array of predicted labels\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for test_point in X_test:\n",
    "        pred = knn_predict_single(X_train, y_train, test_point, k)\n",
    "        predictions.append(pred)\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Accuracy score (0 to 1)\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "print(\"✓ KNN functions implemented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3bda2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test KNN with k=5 on original (unscaled) data\n",
    "k_default = 5\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"TESTING KNN WITH k={k_default} (Original/Unscaled Data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "t_start = time.time()\n",
    "y_pred_original = knn_predict(X_train, y_train, X_test, k=k_default)\n",
    "t_end = time.time()\n",
    "\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "time_original = t_end - t_start\n",
    "\n",
    "print(f\"\\n✓ Prediction completed!\")\n",
    "print(f\"  Accuracy: {accuracy_original:.4f} ({accuracy_original*100:.2f}%)\")\n",
    "print(f\"  Prediction time: {time_original:.4f} seconds\")\n",
    "print(f\"  Predictions shape: {y_pred_original.shape}\")\n",
    "\n",
    "# Show confusion information\n",
    "print(f\"\\nSample predictions (first 10):\")\n",
    "print(f\"  True:      {y_test[:10]}\")\n",
    "print(f\"  Predicted: {y_pred_original[:10]}\")\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "for class_label in np.unique(y_test):\n",
    "    mask = y_test == class_label\n",
    "    class_acc = accuracy_score(y_test[mask], y_pred_original[mask])\n",
    "    print(f\"  Class {class_label} accuracy: {class_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ff093",
   "metadata": {},
   "source": [
    "# Problem 2 — Experimentation with Scaling\n",
    "\n",
    "## Compare Original vs Scaled Features\n",
    "\n",
    "KNN is sensitive to feature scales because it uses distance metrics. Features with larger ranges can dominate the distance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a312e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement standardization (z-score normalization) from scratch\n",
    "def standardize_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Standardize features using z-score normalization.\n",
    "    Fit on training data, transform both train and test.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: Training feature matrix\n",
    "        X_test: Test feature matrix\n",
    "    \n",
    "    Returns:\n",
    "        X_train_scaled, X_test_scaled: Standardized matrices\n",
    "    \"\"\"\n",
    "    # Calculate mean and std from training data only\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    std_safe = np.where(std == 0, 1, std)\n",
    "    \n",
    "    # Standardize\n",
    "    X_train_scaled = (X_train - mean) / std_safe\n",
    "    X_test_scaled = (X_test - mean) / std_safe\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Scale the features\n",
    "X_train_scaled, X_test_scaled = standardize_features(X_train, X_test)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE SCALING (Standardization):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Original X_train - Mean: {X_train.mean(axis=0)[:3]}... \")\n",
    "print(f\"Original X_train - Std:  {X_train.std(axis=0)[:3]}...\")\n",
    "print(f\"\\nScaled X_train - Mean: {X_train_scaled.mean(axis=0)[:3]}...\")\n",
    "print(f\"Scaled X_train - Std:  {X_train_scaled.std(axis=0)[:3]}...\")\n",
    "print(f\"\\n✓ Features scaled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cafac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test KNN on scaled data\n",
    "print(\"=\"*80)\n",
    "print(f\"TESTING KNN WITH k={k_default} (Scaled Data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "t_start = time.time()\n",
    "y_pred_scaled = knn_predict(X_train_scaled, y_train, X_test_scaled, k=k_default)\n",
    "t_end = time.time()\n",
    "\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "time_scaled = t_end - t_start\n",
    "\n",
    "print(f\"\\n✓ Prediction completed!\")\n",
    "print(f\"  Accuracy: {accuracy_scaled:.4f} ({accuracy_scaled*100:.2f}%)\")\n",
    "print(f\"  Prediction time: {time_scaled:.4f} seconds\")\n",
    "\n",
    "# Per-class accuracy\n",
    "for class_label in np.unique(y_test):\n",
    "    mask = y_test == class_label\n",
    "    class_acc = accuracy_score(y_test[mask], y_pred_scaled[mask])\n",
    "    print(f\"  Class {class_label} accuracy: {class_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa296aef",
   "metadata": {},
   "source": [
    "## Comparative Analysis: Original vs Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "comparison = pd.DataFrame({\n",
    "    'Dataset': ['Original', 'Scaled'],\n",
    "    'Accuracy': [accuracy_original, accuracy_scaled],\n",
    "    'Prediction Time (s)': [time_original, time_scaled],\n",
    "    'Improvement': ['Baseline', f\"{((accuracy_scaled - accuracy_original) / accuracy_original * 100):+.2f}%\"]\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARISON: ORIGINAL vs SCALED\")\n",
    "print(\"=\"*80)\n",
    "display(comparison)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(['Original', 'Scaled'], [accuracy_original, accuracy_scaled], \n",
    "            color=['skyblue', 'lightcoral'])\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title(f'Accuracy Comparison (k={k_default})')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate([accuracy_original, accuracy_scaled]):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Time comparison\n",
    "axes[1].bar(['Original', 'Scaled'], [time_original, time_scaled], \n",
    "            color=['skyblue', 'lightcoral'])\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].set_title('Prediction Time Comparison')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate([time_original, time_scaled]):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}s', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISCUSSION:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "**Why Scaling Matters for KNN:**\n",
    "\n",
    "1. **Distance Metric Sensitivity:** KNN uses Euclidean distance. Features with larger \n",
    "   numeric ranges (e.g., Glucose: 0-200) dominate over smaller ranges (e.g., Pregnancies: 0-17).\n",
    "\n",
    "2. **Equal Feature Contribution:** Scaling ensures all features contribute equally to \n",
    "   distance calculations, preventing bias toward high-magnitude features.\n",
    "\n",
    "3. **Performance Impact:** \n",
    "   - If accuracy improves: Features were on different scales, scaling helped.\n",
    "   - If accuracy stays similar: Features may already be comparable, or k needs tuning.\n",
    "   - If accuracy drops: Scaling may amplify noise in weak features.\n",
    "\n",
    "4. **Computational Cost:** Scaling doesn't significantly affect prediction time for \n",
    "   our implementation (dominated by distance calculations).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19193829",
   "metadata": {},
   "source": [
    "# Problem 3 — Experimentation with k Values (1 to 15)\n",
    "\n",
    "Test KNN performance for k = 1, 2, 3, ..., 15 on both original and scaled datasets.\n",
    "Record accuracy and prediction time for each k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with k from 1 to 15\n",
    "k_values = list(range(1, 16))\n",
    "\n",
    "results_original_k = []\n",
    "results_scaled_k = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENTING WITH DIFFERENT k VALUES (1-15)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nTesting k={k}...\")\n",
    "    \n",
    "    # Original data\n",
    "    t_start = time.time()\n",
    "    y_pred_orig = knn_predict(X_train, y_train, X_test, k=k)\n",
    "    t_end = time.time()\n",
    "    acc_orig = accuracy_score(y_test, y_pred_orig)\n",
    "    time_orig = t_end - t_start\n",
    "    results_original_k.append({\n",
    "        'k': k,\n",
    "        'accuracy': acc_orig,\n",
    "        'time': time_orig\n",
    "    })\n",
    "    \n",
    "    # Scaled data\n",
    "    t_start = time.time()\n",
    "    y_pred_scal = knn_predict(X_train_scaled, y_train, X_test_scaled, k=k)\n",
    "    t_end = time.time()\n",
    "    acc_scal = accuracy_score(y_test, y_pred_scal)\n",
    "    time_scal = t_end - t_start\n",
    "    results_scaled_k.append({\n",
    "        'k': k,\n",
    "        'accuracy': acc_scal,\n",
    "        'time': time_scal\n",
    "    })\n",
    "    \n",
    "    print(f\"  Original - Accuracy: {acc_orig:.4f}, Time: {time_orig:.3f}s\")\n",
    "    print(f\"  Scaled   - Accuracy: {acc_scal:.4f}, Time: {time_scal:.3f}s\")\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_results_orig = pd.DataFrame(results_original_k)\n",
    "df_results_scal = pd.DataFrame(results_scaled_k)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nOriginal Dataset:\")\n",
    "display(df_results_orig)\n",
    "\n",
    "print(\"\\nScaled Dataset:\")\n",
    "display(df_results_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa6ddb",
   "metadata": {},
   "source": [
    "## Visualize Results: k vs Accuracy and k vs Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388996e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot k vs Accuracy and k vs Time\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: k vs Accuracy\n",
    "axes[0].plot(df_results_orig['k'], df_results_orig['accuracy'], \n",
    "             marker='o', linewidth=2, label='Original', color='blue')\n",
    "axes[0].plot(df_results_scal['k'], df_results_scal['accuracy'], \n",
    "             marker='s', linewidth=2, label='Scaled', color='red')\n",
    "axes[0].set_xlabel('k (Number of Neighbors)', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('k vs Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(k_values)\n",
    "\n",
    "# Plot 2: k vs Time\n",
    "axes[1].plot(df_results_orig['k'], df_results_orig['time'], \n",
    "             marker='o', linewidth=2, label='Original', color='blue')\n",
    "axes[1].plot(df_results_scal['k'], df_results_scal['time'], \n",
    "             marker='s', linewidth=2, label='Scaled', color='red')\n",
    "axes[1].set_xlabel('k (Number of Neighbors)', fontsize=12)\n",
    "axes[1].set_ylabel('Prediction Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('k vs Prediction Time', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(k_values)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca7060",
   "metadata": {},
   "source": [
    "## Analysis: Identify Optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d696ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k for each dataset\n",
    "best_k_orig = df_results_orig.loc[df_results_orig['accuracy'].idxmax()]\n",
    "best_k_scal = df_results_scal.loc[df_results_scal['accuracy'].idxmax()]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OPTIMAL k VALUES:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal Dataset:\")\n",
    "print(f\"  Best k: {int(best_k_orig['k'])}\")\n",
    "print(f\"  Accuracy: {best_k_orig['accuracy']:.4f}\")\n",
    "print(f\"  Time: {best_k_orig['time']:.4f}s\")\n",
    "\n",
    "print(f\"\\nScaled Dataset:\")\n",
    "print(f\"  Best k: {int(best_k_scal['k'])}\")\n",
    "print(f\"  Accuracy: {best_k_scal['accuracy']:.4f}\")\n",
    "print(f\"  Time: {best_k_scal['time']:.4f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISCUSSION:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "**How k Affects Accuracy and Computational Cost:**\n",
    "\n",
    "1. **Small k (k=1, 2, 3):**\n",
    "   - **Accuracy:** Can be high but sensitive to noise and outliers. Overfitting risk.\n",
    "   - **Decision Boundary:** Complex, non-smooth boundaries.\n",
    "   - **Variance:** High variance, low bias.\n",
    "\n",
    "2. **Moderate k (k=5-10):**\n",
    "   - **Accuracy:** Often provides good balance.\n",
    "   - **Decision Boundary:** Smoother, more generalized.\n",
    "   - **Bias-Variance:** Balanced trade-off.\n",
    "\n",
    "3. **Large k (k>10):**\n",
    "   - **Accuracy:** May decrease if too large (underfitting).\n",
    "   - **Decision Boundary:** Very smooth, may miss local patterns.\n",
    "   - **Variance:** Low variance, high bias.\n",
    "\n",
    "4. **Computational Cost:**\n",
    "   - **Distance Calculation:** Dominant cost (O(n*d) per query, where n=training size, d=features).\n",
    "   - **k Impact:** Sorting/voting adds O(k log k), but negligible compared to distance computation.\n",
    "   - **Our Results:** Time relatively constant across k values (dominated by distance calculations).\n",
    "\n",
    "5. **Optimal k Selection:**\n",
    "   - Choose k with highest validation/test accuracy.\n",
    "   - Prefer odd k for binary classification (avoids ties).\n",
    "   - Use cross-validation for robust selection.\n",
    "   - Consider both accuracy and computational constraints.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b4925",
   "metadata": {},
   "source": [
    "# Problem 4 — Additional Questions (Optional but Recommended)\n",
    "\n",
    "## Challenges of KNN for Large Datasets and High-Dimensional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46acdb34",
   "metadata": {},
   "source": [
    "### 1. Challenges of Using KNN\n",
    "\n",
    "#### **For Large Datasets:**\n",
    "\n",
    "**a) Computational Complexity:**\n",
    "- **Training:** O(1) - lazy learning (no explicit training phase)\n",
    "- **Prediction:** O(n × d) per query, where:\n",
    "  - n = number of training samples\n",
    "  - d = number of features\n",
    "- **Problem:** Linear scaling with dataset size makes prediction slow for large n\n",
    "\n",
    "**b) Memory Requirements:**\n",
    "- Must store entire training dataset in memory\n",
    "- Space complexity: O(n × d)\n",
    "- **Problem:** Memory consumption scales linearly with data size\n",
    "\n",
    "**c) Prediction Latency:**\n",
    "- Each query requires computing distance to ALL training points\n",
    "- Real-time applications become impractical with large datasets\n",
    "- **Example:** For 1 million samples, each prediction requires 1 million distance calculations\n",
    "\n",
    "---\n",
    "\n",
    "#### **For High-Dimensional Data:**\n",
    "\n",
    "**a) Curse of Dimensionality:**\n",
    "- As dimensions increase, data becomes sparse in the feature space\n",
    "- **Problem:** All points become \"far\" from each other, making \"nearest\" meaningless\n",
    "- Distance concentration: max_dist/min_dist → 1 as d → ∞\n",
    "\n",
    "**b) Irrelevant Features:**\n",
    "- Many features may be noise or irrelevant\n",
    "- **Problem:** Distance calculations are polluted by non-informative dimensions\n",
    "- Equal weighting of all features can degrade performance\n",
    "\n",
    "**c) Computational Burden:**\n",
    "- Distance calculation cost grows with dimensions: O(d)\n",
    "- High memory usage: O(n × d)\n",
    "- **Problem:** Both time and space complexity increase\n",
    "\n",
    "**d) Loss of Local Structure:**\n",
    "- Nearest neighbors in high dimensions may not be meaningfully \"near\"\n",
    "- **Problem:** KNN's fundamental assumption breaks down\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ebf8c",
   "metadata": {},
   "source": [
    "### 2. Strategies to Improve KNN Efficiency\n",
    "\n",
    "#### **A. Spatial Indexing Structures**\n",
    "\n",
    "**1. KD-Tree (K-Dimensional Tree):**\n",
    "- **How it works:** Binary tree that recursively partitions space along different dimensions\n",
    "- **Complexity:** O(log n) average query time for low dimensions\n",
    "- **Limitation:** Performance degrades to O(n) for d > 20 (curse of dimensionality)\n",
    "- **Best for:** Low-dimensional data (d < 10-20)\n",
    "\n",
    "**2. Ball Tree:**\n",
    "- **How it works:** Organizes data into nested hyperspheres\n",
    "- **Complexity:** O(log n) query time, more robust than KD-tree for higher dimensions\n",
    "- **Advantage:** Better than KD-tree for d > 10\n",
    "- **Best for:** Moderate-dimensional data\n",
    "\n",
    "**3. Cover Tree:**\n",
    "- **How it works:** Hierarchical data structure with distance-based levels\n",
    "- **Complexity:** O(log n) query time with theoretical guarantees\n",
    "- **Advantage:** Adapts to intrinsic dimensionality of data\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Approximate Nearest Neighbors (ANN)**\n",
    "\n",
    "Trade exact neighbors for speed with controlled accuracy loss:\n",
    "\n",
    "**1. Locality-Sensitive Hashing (LSH):**\n",
    "- **How it works:** Hash similar items to same buckets using special hash functions\n",
    "- **Complexity:** O(1) to O(log n) query time\n",
    "- **Advantage:** Scales to very large datasets\n",
    "- **Use case:** Image retrieval, duplicate detection\n",
    "\n",
    "**2. HNSW (Hierarchical Navigable Small World):**\n",
    "- **How it works:** Multi-layer graph structure for approximate search\n",
    "- **Complexity:** O(log n) query time\n",
    "- **Advantage:** State-of-the-art speed/accuracy trade-off\n",
    "- **Use case:** Modern vector databases, semantic search\n",
    "\n",
    "**3. Product Quantization:**\n",
    "- **How it works:** Compress vectors using learned codebooks\n",
    "- **Advantage:** Reduces memory footprint dramatically\n",
    "- **Use case:** Large-scale image/video search\n",
    "\n",
    "---\n",
    "\n",
    "#### **C. Dimensionality Reduction**\n",
    "\n",
    "**1. Principal Component Analysis (PCA):**\n",
    "- **How it works:** Project data onto top k principal components\n",
    "- **Advantage:** Removes correlated/redundant features, reduces noise\n",
    "- **Typical:** Reduce to 50-100 dimensions retaining 90-95% variance\n",
    "\n",
    "**2. t-SNE / UMAP:**\n",
    "- **How it works:** Non-linear dimensionality reduction preserving local structure\n",
    "- **Use case:** Visualization, preprocessing for KNN\n",
    "\n",
    "**3. Autoencoders:**\n",
    "- **How it works:** Neural network learns compressed representation\n",
    "- **Advantage:** Learns task-specific low-dimensional features\n",
    "\n",
    "---\n",
    "\n",
    "#### **D. Feature Selection**\n",
    "\n",
    "**1. Filter Methods:**\n",
    "- Select features based on statistical tests (correlation, mutual information)\n",
    "- Remove irrelevant/redundant features before KNN\n",
    "\n",
    "**2. Wrapper Methods:**\n",
    "- Use KNN performance to guide feature selection\n",
    "- Forward/backward selection with cross-validation\n",
    "\n",
    "**3. Embedded Methods:**\n",
    "- L1 regularization (Lasso) to identify important features\n",
    "- Tree-based feature importance\n",
    "\n",
    "---\n",
    "\n",
    "#### **E. Algorithm-Level Optimizations**\n",
    "\n",
    "**1. Distance Metric Learning:**\n",
    "- Learn a custom distance metric from data (e.g., Mahalanobis distance)\n",
    "- Weight features by importance\n",
    "- **Advantage:** Improves accuracy without changing k\n",
    "\n",
    "**2. Prototype Selection / Data Reduction:**\n",
    "- **Condensed Nearest Neighbor (CNN):** Keep only boundary samples\n",
    "- **Edited Nearest Neighbor (ENN):** Remove noisy samples\n",
    "- **Advantage:** Reduce n, speeding up prediction\n",
    "\n",
    "**3. Parallel/GPU Computation:**\n",
    "- Parallelize distance calculations across multiple cores/GPUs\n",
    "- Use vectorized operations (NumPy, PyTorch, JAX)\n",
    "- **Advantage:** Linear speedup with hardware\n",
    "\n",
    "**4. Early Stopping:**\n",
    "- Stop searching once k neighbors are found within a distance threshold\n",
    "- Use branch-and-bound pruning in tree structures\n",
    "\n",
    "---\n",
    "\n",
    "#### **F. Hybrid Approaches**\n",
    "\n",
    "**1. Ensemble Methods:**\n",
    "- Combine KNN with other classifiers (Random Forest, SVM)\n",
    "- Use KNN locally and global model for final prediction\n",
    "\n",
    "**2. Preprocessing Pipelines:**\n",
    "- Combine multiple strategies:\n",
    "  - Feature scaling → PCA → KD-Tree → KNN\n",
    "  - Feature selection → LSH → Approximate KNN\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary: Practical Recommendations**\n",
    "\n",
    "| Dataset Characteristics | Recommended Strategy |\n",
    "|------------------------|---------------------|\n",
    "| **n < 10,000, d < 20** | KD-Tree or Ball Tree |\n",
    "| **n > 100,000, d < 50** | HNSW or LSH (ANN) |\n",
    "| **d > 50** | PCA → reduce to d < 30 → KD-Tree |\n",
    "| **d > 100** | Feature selection + PCA + ANN |\n",
    "| **Real-time queries** | Pre-build index (HNSW) + GPU acceleration |\n",
    "| **Memory constrained** | Product quantization + prototype selection |\n",
    "\n",
    "**Key Takeaway:** The best strategy depends on:\n",
    "- Dataset size (n)\n",
    "- Dimensionality (d)\n",
    "- Latency requirements\n",
    "- Accuracy tolerance\n",
    "- Available hardware"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
